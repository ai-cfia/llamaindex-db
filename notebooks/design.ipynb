{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an index using Llamaindex and PostgreSQL\n",
    "\n",
    "The vector store is the main component used in LlamaIndex to query a set of document. It basically contains vector embeddings. By default LlamaIndex builds the vector store in memory which is not so suitable for production. We decided to use instead our PostgreSQL database to host the vector store as a table (`data_llamaindex`) and create a HNSW index on the `embedding` column to allow faster retrievals. The process is shown [here](pgvector_ailab_db.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex integration in finesse\n",
    "\n",
    "We expose the search functionality as a package (`ailab-llamaindex-search`) for reusability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components\n",
    "\n",
    "The diagram below shows how it integrates in `finesse-backend`.\n",
    "\n",
    "![components](../docs/img/components.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startup sequence\n",
    "\n",
    "The following will be integrated into `finesse-backend` startup sequence.\n",
    "\n",
    "![startup sequence](../docs/img/startup_sequence.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search sequence\n",
    "\n",
    "Searching the index involves the following steps.\n",
    "\n",
    "![search sequence](../docs/img/search_sequence.png)\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- A `retriever` object generates embeddings for query strings and matches them with database embeddings for each API search call. It's created on-demand and garbage-collected post-use, enabling scalability in the number of nodes returned.\n",
    "- Returned nodes may reference the same document; post-processing is necessary to remove duplicates before returning results to the user.\n",
    "- Semantic re-ranking might also be needed in the post-processing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex-db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
