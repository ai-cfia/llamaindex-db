{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index==0.9.36\n",
    "!pip install asyncpg==0.29.0\n",
    "%pip install openai --upgrade\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "import logging\n",
    "import sys\n",
    "from collections.abc import Iterator\n",
    "from llama_index.core import ServiceContext, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import textwrap\n",
    "import openai\n",
    "#from llama_index import download_loader\n",
    "# customize textnode - purpose is to add id to each node\n",
    "#from llama_index.schema import TextNode\n",
    "# customize stages of querying  https://docs.llamaindex.ai/en/latest/understanding/querying/querying.html\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.indices.vector_store.retrievers.retriever import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "#from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download query engine from Azure Blob Storage Container\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# Your storage account connection string\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=finessetestblobstorage;AccountKey=;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# The name of your container\n",
    "container_name = \"llamaindex-v1\"\n",
    "\n",
    "# The name of the virtual folder you want to list files from\n",
    "folder_name = \"index\"\n",
    "\n",
    "# Initialize the BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get the container client\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# List all blobs in the specified folder\n",
    "blobs_list = container_client.list_blobs(name_starts_with=folder_name)\n",
    "\n",
    "# List all blobs in the container (at the root)\n",
    "blobs_list = container_client.list_blobs()\n",
    "\n",
    "for blob in blobs_list:\n",
    "    print(\"Blob name: \" + blob.name)\n",
    "    blob_name = blob.name\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    # Download the blob to a local file\n",
    "    download_file_path = \"./index/\" + blob_name\n",
    "    with open(download_file_path, \"wb\") as download_file:\n",
    "        download_file.write(blob_client.download_blob().readall())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query engine from local folder, 'index'\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./index\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# configure retriever for debugging and retrieving metadata \n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=15, # get top k documents.  15 is arbitrary\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of query execute\n",
    "response = query_engine.query(\"How do I import a cat from France to Canada?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top k result into a list, in order of match score\n",
    "top_k_result = []\n",
    "for i in range(15): # arbitrary 15 because similarity_top_k=15 in this example\n",
    "    top_k_result.append(response.source_nodes[i])\n",
    "    \n",
    "# get content\n",
    "response.source_nodes[0].get_content()\n",
    "# get embedding\n",
    "response.source_nodes[0].embedding\n",
    "# get score\n",
    "response.source_nodes[0].get_score()\n",
    "# get customized metadata. In this example, this retrieves chunk_id\n",
    "response.source_nodes[0].metadata"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
