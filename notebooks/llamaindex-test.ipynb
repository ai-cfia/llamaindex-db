{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "import logging\n",
    "import sys\n",
    "from collections.abc import Iterator\n",
    "from sqlalchemy import make_url, create_engine, MetaData\n",
    "from llama_index.core import ServiceContext, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "#from llama_index.vector_stores import PGVectorStore\n",
    "import textwrap\n",
    "import openai\n",
    "\n",
    "# customize textnode - purpose is to add id to each node\n",
    "#from llama_index.schema import TextNode\n",
    "# customize stages of querying  https://docs.llamaindex.ai/en/latest/understanding/querying/querying.html\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.indices.vector_store.retrievers.retriever import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "azure_endpoint = \"\"\n",
    "api_version = \"2023-07-01-preview\"\n",
    "\n",
    "# create llm and embedding model apis\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=\"ailab-llm\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"ada\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever for debugging and retrieving metadata \n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=15,\n",
    ")\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q['llamaindex_response'] = ''\n",
    "df_q['llamaindex_top_15_doc'] = ''\n",
    "df_q['llamaindex_top_15_doc_id'] = ''\n",
    "df_q['llamaindex_answer_placement'] = ''\n",
    "df_result = df_q.copy()\n",
    "def find_string_position(string_list, target_string):\n",
    "    \"\"\"\n",
    "    Finds the position of a target string in a list of strings.\n",
    "\n",
    "    Parameters:\n",
    "    - string_list: A list of strings to search through.\n",
    "    - target_string: The string to find within the list.\n",
    "\n",
    "    Returns:\n",
    "    - The index (position) of the target string in the list, or -1 if not found.\n",
    "    \"\"\"\n",
    "    for index, string in enumerate(string_list):\n",
    "        if string == target_string:\n",
    "            return index\n",
    "    return -1\n",
    "for i in range(len(df_result)):\n",
    "    print('i', i)\n",
    "    # query\n",
    "    response = query_engine.query(df_result.iloc[i]['question'])\n",
    "    print(response.get_formatted_sources())\n",
    "    print(\"query was:\", df_result.iloc[i]['question'])\n",
    "    print(\"answer was:\", response)\n",
    "    # get top k result into a list, in order of match score\n",
    "    top_k_result = []\n",
    "    top_k_result_id = []\n",
    "    for j in range(15):\n",
    "        top_k_result.append(response.source_nodes[j])\n",
    "        top_k_result_id.append(response.source_nodes[j].metadata['id_'])\n",
    "    #print('top_k_result', top_k_result)\n",
    "    # get customized metadata\n",
    "    #response.source_nodes[0].metadata\n",
    "    df_result.at[i,'llamaindex_response'] = response\n",
    "    df_result.at[i,'llamaindex_top_15_doc'] = top_k_result\n",
    "    df_result.at[i,'llamaindex_top_15_doc_id'] = top_k_result_id\n",
    "    df_result.at[i,'llamaindex_answer_placement'] = find_string_position(top_k_result_id, df_result['chunk_id'].iloc[i])\n",
    "    #print('df_result-------------------', df_result.iloc[i])\n",
    "df = df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 10000\n",
    "df[['question','llamaindex_answer_placement','llamaindex_response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/good_qna_llamaindex_answer.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
