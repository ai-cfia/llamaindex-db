{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bccd47fc",
      "metadata": {
        "id": "bccd47fc"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/vector_stores/postgres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9602292f",
      "metadata": {},
      "source": [
        "# Postgres Vector Store\n",
        "\n",
        "This notebook shows how we can create a llamaindex in PostgresSQL (PGVector) as opposed to in-memory, from data crawled directly from `inspection.canada.ca`. \n",
        "\n",
        "From few informal tests on a set of 500 pages, the results were accurate. We will need to conduct formal tests on the whole document set to confirm this.\n",
        "\n",
        "Queries are also faster: PGVector<500ms vs in-memory>3000ms. \n",
        "\n",
        "As can been from the indexing process, new individual documents can be added to the index on the go. Similar methods are available to remove documents. This opens the door to CRUD capabilities. The recommendation is to use this method to build a new index from scratch.\n",
        "\n",
        "**Notes:**\n",
        "- These tests are conducted on a local machine, so they don't consider remote db round trip delays.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2fc9c18",
      "metadata": {
        "id": "d2fc9c18"
      },
      "outputs": [],
      "source": [
        "%pip install -r ../../requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c2d1c538",
      "metadata": {
        "id": "c2d1c538"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import logging\n",
        "import sys\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.postgres import PGVectorStore\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.core import Settings\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.storage.index_store.postgres import PostgresIndexStore\n",
        "from llama_index.storage.docstore.postgres import PostgresDocumentStore\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from pprint import pprint\n",
        "import psycopg\n",
        "from llama_index.core.schema import Document\n",
        "import pickle\n",
        "from sqlalchemy import make_url\n",
        "from datetime import datetime\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from bs4 import BeautifulSoup\n",
        "from llama_index.core.extractors import QuestionsAnsweredExtractor\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Uncomment to see debug logs\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7ef96d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_to_pickle(data, filename):\n",
        "    with open(filename, \"wb\") as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "def load_from_pickle(filename):\n",
        "    with open(filename, \"rb\") as file:\n",
        "        return pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c71b6d",
      "metadata": {
        "id": "26c71b6d"
      },
      "source": [
        "### Setup LLM and Embed Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67b86621",
      "metadata": {
        "id": "67b86621"
      },
      "outputs": [],
      "source": [
        "llm = AzureOpenAI(\n",
        "    model=\"gpt-4\",\n",
        "    deployment_name=\"ailab-llm\",\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
        "    api_version=os.getenv(\"API_VERSION\"),\n",
        ")\n",
        "\n",
        "embed_model = AzureOpenAIEmbedding(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    deployment_name=\"ada\",\n",
        "    api_key=os.getenv(\"API_KEY\"),\n",
        "    azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
        "    api_version=os.getenv(\"API_VERSION\"),\n",
        ")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45551f5d",
      "metadata": {},
      "source": [
        "### Creating a sample document collection\n",
        "\n",
        "#### Get a list of all urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "fb97d6a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['https://inspection.canada.ca/preventive-controls/sampling-procedures/eng/1518033335104/1528203403149',\n",
            " 'https://inspection.canada.ca/eng/1664715510668/1664715511012',\n",
            " 'https://inspection.canada.ca/plant-health/potatoes/potato-varieties/norland/eng/1312587385821/1312587385822',\n",
            " 'https://inspection.canada.ca/controles-preventifs/lutte-antiparasitaire/fra/1511206644150/1528205213795',\n",
            " 'https://inspection.canada.ca/eng/1653077788730/1653077789089']\n"
          ]
        }
      ],
      "source": [
        "conn_string = (\n",
        "    f\"dbname={os.getenv('DB_NAME')} \"\n",
        "    f\"user={os.getenv('DB_USER')} \"\n",
        "    f\"password={os.getenv('DB_PASSWORD')} \"\n",
        "    f\"host={os.getenv('DB_HOST')} \"\n",
        "    f\"port={os.getenv('DB_PORT')}\"\n",
        ")\n",
        "query = \"\"\"\n",
        "    SELECT c.url\n",
        "    FROM louis_v005.crawl as c\n",
        "    \"\"\"\n",
        "with psycopg.connect(conn_string) as conn:\n",
        "    with conn.cursor() as cur:\n",
        "        results = cur.execute(query).fetchall()\n",
        "        urls = [r[0] for r in results]\n",
        "\n",
        "pprint(urls[0:5])\n",
        "save_to_pickle(urls, \"urls.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6618cbf5",
      "metadata": {},
      "source": [
        "#### Create a sample of nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f0c8fad3",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AiLabWebPageReader(SimpleWebPageReader):\n",
        "    \"\"\"AiLab web page reader.\n",
        "\n",
        "    Reads pages from the web.\n",
        "\n",
        "    Args:\n",
        "        html_to_text (bool): Whether to convert HTML to text.\n",
        "            Requires `html2text` package.\n",
        "        metadata_fn (Optional[Callable[[str], Dict]]): A function that takes in\n",
        "            a URL and returns a dictionary of metadata.\n",
        "            Default is None.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def class_name(cls) -> str:\n",
        "        return \"AiLabWebPageReader\"\n",
        "    \n",
        "    def load_data(self, urls: list[str]) -> list[Document]:\n",
        "        \"\"\"Load data from the input directory.\n",
        "\n",
        "        Args:\n",
        "            urls (List[str]): List of URLs to scrape.\n",
        "\n",
        "        Returns:\n",
        "            List[Document]: List of documents.\n",
        "\n",
        "        \"\"\"\n",
        "        if not isinstance(urls, list):\n",
        "            raise ValueError(\"urls must be a list of strings.\")\n",
        "        documents = []\n",
        "        for url in urls:\n",
        "            response = requests.get(url, headers=None).text\n",
        "\n",
        "            metadata: Optional[dict] = None\n",
        "            if self._metadata_fn is not None:\n",
        "                metadata = self._metadata_fn(url, response)\n",
        "            \n",
        "            if self.html_to_text:\n",
        "                import html2text\n",
        "                response = html2text.html2text(response)\n",
        "\n",
        "            documents.append(Document(text=response, id_=url, metadata=metadata or {}))\n",
        "\n",
        "        return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b126c54",
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = load_from_pickle(\"urls.pkl\")\n",
        "nb_pages = 500\n",
        "\n",
        "def metadata_fn(*args) -> dict:\n",
        "    url: str = args[0]\n",
        "    html: str = args[1]\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": soup.title.string.strip(),\n",
        "        \"last_crawled\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "        \"lang\": \"fr\" if \"/fra/\" in url else \"en\",\n",
        "    }\n",
        "\n",
        "documents = AiLabWebPageReader(html_to_text=True, metadata_fn=metadata_fn).load_data(\n",
        "    urls[0:nb_pages]\n",
        ")\n",
        "parser = SentenceSplitter()\n",
        "nodes = parser.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "588efbf7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500\n",
            "2783\n"
          ]
        }
      ],
      "source": [
        "pprint(len(documents))\n",
        "pprint(len(nodes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd24f0a",
      "metadata": {
        "id": "7bd24f0a"
      },
      "source": [
        "### Create the Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e6d61e73",
      "metadata": {
        "id": "e6d61e73"
      },
      "outputs": [],
      "source": [
        "# connection_string=conn_string\n",
        "connection_string = \"postgresql://postgres:testpwd@localhost:5432\"\n",
        "db_name = \"llamaindex_db_crawl\"\n",
        "\n",
        "with psycopg.connect(connection_string) as conn:\n",
        "    conn.autocommit = True\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(psycopg.sql.SQL(\"DROP DATABASE IF EXISTS {}\").format(psycopg.sql.Identifier(db_name)))\n",
        "        cur.execute(psycopg.sql.SQL(\"CREATE DATABASE {}\").format(psycopg.sql.Identifier(db_name)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0232fd1",
      "metadata": {
        "id": "c0232fd1"
      },
      "source": [
        "### Create the indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8731da62",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "55d642be4afd424dbeddcc98a6313baa",
            "4bedc19d901346dbafbeff1d25638562"
          ]
        },
        "id": "8731da62",
        "outputId": "bc5f1134-e829-4357-9caa-f3012cc011be"
      },
      "outputs": [],
      "source": [
        "url = make_url(connection_string)\n",
        "vector_store = PGVectorStore.from_params(\n",
        "    database=db_name,\n",
        "    host=url.host,\n",
        "    password=url.password,\n",
        "    port=url.port,\n",
        "    user=url.username,\n",
        "    embed_dim=1536,\n",
        ")\n",
        "\n",
        "document_store = PostgresDocumentStore.from_params(    \n",
        "    database=db_name,\n",
        "    host=url.host,\n",
        "    password=url.password,\n",
        "    port=url.port,\n",
        "    user=url.username,\n",
        ")\n",
        "\n",
        "index_store = PostgresIndexStore.from_params(\n",
        "    database=db_name,\n",
        "    host=url.host,\n",
        "    password=url.password,\n",
        "    port=url.port,\n",
        "    user=url.username,\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    docstore=document_store,\n",
        "    index_store=index_store, \n",
        "    vector_store=vector_store, \n",
        ")\n",
        "\n",
        "storage_context.docstore.add_documents(nodes)\n",
        "\n",
        "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
      "metadata": {
        "id": "8ee4473a-094f-4d0a-a825-e1213db07240"
      },
      "source": [
        "### Query the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddf1cd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "pprint(urls[0:nb_pages])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2bcc07",
      "metadata": {
        "id": "0a2bcc07"
      },
      "outputs": [],
      "source": [
        "retriever = index.as_retriever(similarity_top_k=5)\n",
        "nodes = retriever.retrieve(\"tissus interdits dans la chaine alimentaire\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50f7b57",
      "metadata": {},
      "source": [
        "### Testing\n",
        "\n",
        "#### Generating a question from a random url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dad4fdcd",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.09s/it]\n"
          ]
        }
      ],
      "source": [
        "# urls = load_from_pickle(\"urls.pkl\")\n",
        "# eng_urls = [url for url in urls if \"/fra/\" not in url]\n",
        "# random_url = random.choice(eng_urls)\n",
        "random_url = urls[0]\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data([random_url])\n",
        "assert len(documents)==1\n",
        "extractor = QuestionsAnsweredExtractor(questions=1)\n",
        "questions = await extractor.aextract(documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b891ba3a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "url https://inspection.canada.ca/preventive-controls/sampling-procedures/eng/1518033335104/1528203403149\n",
            "What are the steps and considerations for collecting environmental samples for microbial testing in a food production setting according to the Canadian Food Inspection Agency?\n"
          ]
        }
      ],
      "source": [
        "print(\"url\", random_url)\n",
        "question = questions[0][\"questions_this_excerpt_can_answer\"].removeprefix(\"Question: \")\n",
        "print(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1077b224",
      "metadata": {},
      "outputs": [],
      "source": [
        "# connection_string=conn_string\n",
        "connection_string = \"postgresql://postgres:testpwd@localhost:5432\"\n",
        "db_name = \"llamaindex_db_crawl\"\n",
        "url = make_url(connection_string)\n",
        "vector_store = PGVectorStore.from_params(\n",
        "    database=db_name,\n",
        "    host=url.host,\n",
        "    password=url.password,\n",
        "    port=url.port,\n",
        "    user=url.username,\n",
        "    embed_dim=1536,\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
        "retriever = index.as_retriever(similarity_top_k=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d316adbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elapsed time: 0.36 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "nodes = retriever.retrieve(question)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d0416779",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position: 1 Sampling procedures - Canadian Food Inspection Agency\n",
            "Position: 2 Sampling procedures - Canadian Food Inspection Agency\n",
            "Position: 3 Sampling procedures - Canadian Food Inspection Agency\n",
            "Position: 4 Sampling procedures - Canadian Food Inspection Agency\n",
            "Position: 5 Sampling procedures - Canadian Food Inspection Agency\n",
            "Position: 7 Sampling procedures - Canadian Food Inspection Agency\n",
            "Position: 9 Sampling procedures - Canadian Food Inspection Agency\n",
            "Position: 12 Sampling procedures - Canadian Food Inspection Agency\n"
          ]
        }
      ],
      "source": [
        "found = False\n",
        "for i, n in enumerate(nodes):\n",
        "    if n.metadata[\"url\"] == random_url:\n",
        "        found = True\n",
        "        print(f\"Position: {i+1}\", n.metadata[\"title\"])\n",
        "\n",
        "if not found:\n",
        "    print(\"Right:\", random_url)\n",
        "    for n in nodes:\n",
        "        print(\"Wrong: \", n.metadata[\"url\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
